{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, LSTM, Dropout, Bidirectional, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(r'C:/Users/D070651/Documents/Uni/thesis/Project_implementation/ActiveLearning/Datasets/DBpedia/DBPEDIA_train.csv').sample(5000, random_state=42)\n",
    "df_val=pd.read_csv(r'C:/Users/D070651/Documents/Uni/thesis/Project_implementation/ActiveLearning/Datasets/DBpedia/DBPEDIA_val.csv').sample(5000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Work', 'Place', 'Species', 'Agent', 'Event', 'UnitOfWork',\n",
       "       'SportsSeason', 'TopicalConcept', 'Device'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"l1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text       l1  \\\n",
      "174655  The Musical Jigsaw Play is a 1994 family music...     Work   \n",
      "24221   Water Eaton House Bridge is a footbridge acros...    Place   \n",
      "18945   The European Coatings Journal is an English-la...     Work   \n",
      "192759  Penicillium viridicatum is a psychrophilic spe...  Species   \n",
      "197276  Newark Beth Israel Medical Center, previously ...    Place   \n",
      "...                                                   ...      ...   \n",
      "170420  Cleo, Camping, Emmanuelle and Dick is a 1998 p...     Work   \n",
      "150753  Paddy O'Keeffe (born 1864) was an Irish hurler...    Agent   \n",
      "47272   Douglas Lee Beaudoin (born May 15, 1954 in Dic...    Agent   \n",
      "228871  Sergeant John Pointon Beech (May 1, 1844 – Nov...    Agent   \n",
      "118277  Nicholas Viscardi (October 20, 1920 – November...    Agent   \n",
      "\n",
      "                            l2                      l3  \n",
      "174655             WrittenWork                    Play  \n",
      "24221    RouteOfTransportation                  Bridge  \n",
      "18945     PeriodicalLiterature                Magazine  \n",
      "192759               Eukaryote                  Fungus  \n",
      "197276                Building                Hospital  \n",
      "...                        ...                     ...  \n",
      "170420             WrittenWork                    Play  \n",
      "150753                 Athlete       GaelicGamesPlayer  \n",
      "47272   GridironFootballPlayer  AmericanFootballPlayer  \n",
      "228871                  Person          MilitaryPerson  \n",
      "118277                  Artist           ComicsCreator  \n",
      "\n",
      "[5000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(oov_token=\"'oov'\")\n",
    "tokenizer.fit_on_texts(df_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "train_X = pad_sequences(tokenizer.texts_to_sequences(df_train['text']), maxlen=maxlen)\n",
    "val_X = pad_sequences(tokenizer.texts_to_sequences(df_val['text']), maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "enc.fit(df_train[\"l1\"])\n",
    "train_Y = to_categorical(enc.transform(df_train[\"l1\"]))\n",
    "val_Y = to_categorical(enc.transform(df_val[\"l1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors \n"
     ]
    }
   ],
   "source": [
    "glove_dir=\"C:/Users/D070651/Documents/Uni/thesis/Project_implementation/ActiveLearning/Datasets\"\n",
    "\n",
    "embedding_index = {}\n",
    "f = open(os.path.join(glove_dir,'glove.6B.100d.txt'),encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors ' % len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words,embedding_dim))\n",
    "\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx]=embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, Y, pool):\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(9, activation=\"softmax\"))\n",
    "    model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X, Y, epochs=20, batch_size=64, verbose=0)\n",
    "    \n",
    "    val_acc = accuracy_score([np.argmax(p) for p in val_Y], [np.argmax(p) for p in model.predict(val_X)])\n",
    "    pool_predictions = model.predict(pool)\n",
    "    return val_acc, pool_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, Y):\n",
    "        self._X = X\n",
    "        self._Y = Y\n",
    "        self._labeled = np.array([False for _ in range(0, len(self._X))])\n",
    "    \n",
    "    @property\n",
    "    def pool(self):\n",
    "        return self._X\n",
    "    \n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._X[self._labeled]\n",
    "    \n",
    "    @property\n",
    "    def Y(self):\n",
    "        return self._Y[self._labeled]\n",
    "    \n",
    "    def random_sampling(self, batch_size):\n",
    "        not_labeled = np.where(self._labeled == False)[0]\n",
    "        new_labels = []\n",
    "        while len(new_labels) < batch_size:\n",
    "            r = random.randrange(0, len(not_labeled))\n",
    "            if not_labeled[r] not in new_labels:\n",
    "                new_labels.append(not_labeled[r])\n",
    "        self._labeled[new_labels] = True\n",
    "    \n",
    "    def lc_sampling(self, batch_size, predictions):\n",
    "        lc = sorted([(1 - p[np.argmax(p)], i) for i, p in enumerate(predictions)], reverse=True)\n",
    "        self._label_batch(lc, batch_size)\n",
    "                \n",
    "    def margin_sampling(self, batch_size, predictions):\n",
    "        ms = sorted([(p[np.argsort(p)[-1]] - p[np.argsort(p)[-2]], i) for i, p in enumerate(predictions)])\n",
    "        self._label_batch(ms, batch_size)\n",
    "    \n",
    "    def entropy_sampling(self, batch_size, predictions):\n",
    "        es = sorted([(entropy(p), i) for i, p in enumerate(predictions)], reverse=True)\n",
    "        self._label_batch(es, batch_size)\n",
    "        \n",
    "    def _label_batch(self, sorted_candidates, batch_size):\n",
    "        i = 0\n",
    "        for _, j in sorted_candidates:\n",
    "            if not self._labeled[j]: #if not already labeled\n",
    "                self._labeled[j] = True\n",
    "                i += 1\n",
    "            if i >= batch_size:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning(query_strategy, seed_size, batch_size, num_steps):\n",
    "    \"\"\"\n",
    "    query_strategy - 'lc' for Least confidence sampling\n",
    "                   - 'ms' for Margin sampling\n",
    "                   - 'es' for Entropy sampling\n",
    "                   - 'rs' for Random sampling\n",
    "    \"\"\"\n",
    "    assert query_strategy in [\"lc\", \"ms\", \"es\", \"rs\"], \"Unknown query strategy\"\n",
    "    accuracies = []\n",
    "    d = Dataset(train_X, train_Y)\n",
    "    d.random_sampling(seed_size)\n",
    "    acc, predictions = train_model(d.X, d.Y, d.pool)\n",
    "    accuracies.append(acc)\n",
    "    for _ in tqdm(range(0, num_steps)):\n",
    "        if query_strategy == \"lc\":\n",
    "            d.lc_sampling(batch_size, predictions)\n",
    "        elif query_strategy == \"ms\":\n",
    "            d.margin_sampling(batch_size, predictions)\n",
    "        elif query_strategy == \"es\":\n",
    "            d.entropy_sampling(batch_size, predictions)\n",
    "        elif query_strategy == \"rs\":\n",
    "            d.random_sampling(batch_size)\n",
    "        acc, predictions = train_model(d.X, d.Y, d.pool)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_size=100\n",
    "batch_size=50\n",
    "num_steps=98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|██████████████████████████████████████████████████████████▌                       | 70/98 [22:52<18:24, 39.46s/it]"
     ]
    }
   ],
   "source": [
    "random_accuracies = active_learning(\"rs\", seed_size, batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_accuracies = active_learning(\"lc\", seed_size, batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_accuracies = active_learning(\"ms\", seed_size, batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_accuracies = active_learning(\"es\", seed_size, batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size), random_accuracies, color=\"b\", label=\"Random Sampling\")\n",
    "plt.plot(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size), lc_accuracies, color=\"g\", label=\"Least Confidence Sampling\")\n",
    "plt.plot(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size), ms_accuracies, color=\"r\", label=\"Margin Sampling\")\n",
    "plt.plot(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size), es_accuracies, color=\"y\", label=\"Entropy Sampling\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Active Learning on DBPedia Classes Dataset\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Labeled data')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy after labeling 2000 data points¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size) == 2000)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_accuracies[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_accuracies[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of labeled data points required to the reach accuracy of 85%¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return seed_size + idx * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_nearest(random_accuracies, 0.85) #random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_nearest(lc_accuracies, 0.85) #least confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Notebook has been released under the Apache 2.0 open source license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
